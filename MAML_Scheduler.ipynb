{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM0QVuRAAJiMf95/6QGUqPi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mugdha1503/maml_rl/blob/main/MAML_Scheduler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuqxZNWJx0ZG",
        "outputId": "81b47040-a539-4e18-cde0-14b5223eada1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta iteration 1/50 starting...\n",
            "Average reward for this meta-update: -2.95\n",
            "Meta iteration 1/50 complete\n",
            "Meta iteration 2/50 starting...\n",
            "Average reward for this meta-update: -0.02\n",
            "Meta iteration 2/50 complete\n",
            "Meta iteration 3/50 starting...\n",
            "Average reward for this meta-update: 2.70\n",
            "Meta iteration 3/50 complete\n",
            "Meta iteration 4/50 starting...\n",
            "Average reward for this meta-update: -1.92\n",
            "Meta iteration 4/50 complete\n",
            "Meta iteration 5/50 starting...\n",
            "Average reward for this meta-update: 2.89\n",
            "Meta iteration 5/50 complete\n",
            "Meta iteration 6/50 starting...\n",
            "Average reward for this meta-update: 1.26\n",
            "Meta iteration 6/50 complete\n",
            "Meta iteration 7/50 starting...\n",
            "Average reward for this meta-update: 2.59\n",
            "Meta iteration 7/50 complete\n",
            "Meta iteration 8/50 starting...\n",
            "Average reward for this meta-update: 4.00\n",
            "Meta iteration 8/50 complete\n",
            "Meta iteration 9/50 starting...\n",
            "Average reward for this meta-update: 4.63\n",
            "Meta iteration 9/50 complete\n",
            "Meta iteration 10/50 starting...\n",
            "Average reward for this meta-update: 7.57\n",
            "Meta iteration 10/50 complete\n",
            "Meta iteration 11/50 starting...\n",
            "Average reward for this meta-update: 3.90\n",
            "Meta iteration 11/50 complete\n",
            "Meta iteration 12/50 starting...\n",
            "Average reward for this meta-update: 6.96\n",
            "Meta iteration 12/50 complete\n",
            "Meta iteration 13/50 starting...\n",
            "Average reward for this meta-update: 8.87\n",
            "Meta iteration 13/50 complete\n",
            "Meta iteration 14/50 starting...\n",
            "Average reward for this meta-update: 6.63\n",
            "Meta iteration 14/50 complete\n",
            "Meta iteration 15/50 starting...\n",
            "Average reward for this meta-update: 7.49\n",
            "Meta iteration 15/50 complete\n",
            "Meta iteration 16/50 starting...\n",
            "Average reward for this meta-update: 9.71\n",
            "Meta iteration 16/50 complete\n",
            "Meta iteration 17/50 starting...\n",
            "Average reward for this meta-update: 8.71\n",
            "Meta iteration 17/50 complete\n",
            "Meta iteration 18/50 starting...\n",
            "Average reward for this meta-update: 9.98\n",
            "Meta iteration 18/50 complete\n",
            "Meta iteration 19/50 starting...\n",
            "Average reward for this meta-update: 9.57\n",
            "Meta iteration 19/50 complete\n",
            "Meta iteration 20/50 starting...\n",
            "Average reward for this meta-update: 10.43\n",
            "Meta iteration 20/50 complete\n",
            "Meta iteration 21/50 starting...\n",
            "Average reward for this meta-update: 14.07\n",
            "Meta iteration 21/50 complete\n",
            "Meta iteration 22/50 starting...\n",
            "Average reward for this meta-update: 13.07\n",
            "Meta iteration 22/50 complete\n",
            "Meta iteration 23/50 starting...\n",
            "Average reward for this meta-update: 15.01\n",
            "Meta iteration 23/50 complete\n",
            "Meta iteration 24/50 starting...\n",
            "Average reward for this meta-update: 15.30\n",
            "Meta iteration 24/50 complete\n",
            "Meta iteration 25/50 starting...\n",
            "Average reward for this meta-update: 14.60\n",
            "Meta iteration 25/50 complete\n",
            "Meta iteration 26/50 starting...\n",
            "Average reward for this meta-update: 16.52\n",
            "Meta iteration 26/50 complete\n",
            "Meta iteration 27/50 starting...\n",
            "Average reward for this meta-update: 16.25\n",
            "Meta iteration 27/50 complete\n",
            "Meta iteration 28/50 starting...\n",
            "Average reward for this meta-update: 17.54\n",
            "Meta iteration 28/50 complete\n",
            "Meta iteration 29/50 starting...\n",
            "Average reward for this meta-update: 17.79\n",
            "Meta iteration 29/50 complete\n",
            "Meta iteration 30/50 starting...\n",
            "Average reward for this meta-update: 18.67\n",
            "Meta iteration 30/50 complete\n",
            "Meta iteration 31/50 starting...\n",
            "Average reward for this meta-update: 19.86\n",
            "Meta iteration 31/50 complete\n",
            "Meta iteration 32/50 starting...\n",
            "Average reward for this meta-update: 20.16\n",
            "Meta iteration 32/50 complete\n",
            "Meta iteration 33/50 starting...\n",
            "Average reward for this meta-update: 20.34\n",
            "Meta iteration 33/50 complete\n",
            "Meta iteration 34/50 starting...\n",
            "Average reward for this meta-update: 20.35\n",
            "Meta iteration 34/50 complete\n",
            "Meta iteration 35/50 starting...\n",
            "Average reward for this meta-update: 20.40\n",
            "Meta iteration 35/50 complete\n",
            "Meta iteration 36/50 starting...\n",
            "Average reward for this meta-update: 20.48\n",
            "Meta iteration 36/50 complete\n",
            "Meta iteration 37/50 starting...\n",
            "Average reward for this meta-update: 21.95\n",
            "Meta iteration 37/50 complete\n",
            "Meta iteration 38/50 starting...\n",
            "Average reward for this meta-update: 24.27\n",
            "Meta iteration 38/50 complete\n",
            "Meta iteration 39/50 starting...\n",
            "Average reward for this meta-update: 21.54\n",
            "Meta iteration 39/50 complete\n",
            "Meta iteration 40/50 starting...\n",
            "Average reward for this meta-update: 22.19\n",
            "Meta iteration 40/50 complete\n",
            "Meta iteration 41/50 starting...\n",
            "Average reward for this meta-update: 23.38\n",
            "Meta iteration 41/50 complete\n",
            "Meta iteration 42/50 starting...\n",
            "Average reward for this meta-update: 25.10\n",
            "Meta iteration 42/50 complete\n",
            "Meta iteration 43/50 starting...\n",
            "Average reward for this meta-update: 23.06\n",
            "Meta iteration 43/50 complete\n",
            "Meta iteration 44/50 starting...\n",
            "Average reward for this meta-update: 25.39\n",
            "Meta iteration 44/50 complete\n",
            "Meta iteration 45/50 starting...\n",
            "Average reward for this meta-update: 25.01\n",
            "Meta iteration 45/50 complete\n",
            "Meta iteration 46/50 starting...\n",
            "Average reward for this meta-update: 23.42\n",
            "Meta iteration 46/50 complete\n",
            "Meta iteration 47/50 starting...\n",
            "Average reward for this meta-update: 23.10\n",
            "Meta iteration 47/50 complete\n",
            "Meta iteration 48/50 starting...\n",
            "Average reward for this meta-update: 25.88\n",
            "Meta iteration 48/50 complete\n",
            "Meta iteration 49/50 starting...\n",
            "Average reward for this meta-update: 26.28\n",
            "Meta iteration 49/50 complete\n",
            "Meta iteration 50/50 starting...\n",
            "Average reward for this meta-update: 27.80\n",
            "Meta iteration 50/50 complete\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "file_path = '/content/Dataset2.csv'\n",
        "dataset = pd.read_csv(file_path)\n",
        "\n",
        "columns_to_clean = ['P1_RTT', 'P1_CWND', 'P1_inflight', 'P2_RTT', 'P2_CWND', 'P2_inflight']\n",
        "\n",
        "# Clean columns\n",
        "def clean_column(col):\n",
        "    cleaned_col = pd.to_numeric(dataset[col], errors='coerce')\n",
        "    cleaned_col.fillna(cleaned_col.mean(), inplace=True)\n",
        "    return cleaned_col\n",
        "\n",
        "for col in columns_to_clean:\n",
        "    dataset[col] = clean_column(col)\n",
        "\n",
        "# Drop any unnecessary columns\n",
        "dataset = dataset.drop(columns=['Unnamed: 6'], errors='ignore')\n",
        "\n",
        "# Normalize columns\n",
        "def normalize(column):\n",
        "    min_val = column.min()\n",
        "    max_val = column.max()\n",
        "    return (column - min_val) / (max_val - min_val)\n",
        "\n",
        "for col in columns_to_clean:\n",
        "    dataset[col] = normalize(dataset[col])\n",
        "\n",
        "# Define the reward calculation function\n",
        "def calculate_reward(path_1, path_2, action):\n",
        "    weights = [0.5, 0.3, 0.2]\n",
        "    score_1 = sum(w * p for w, p in zip(weights, path_1))\n",
        "    score_2 = sum(w * p for w, p in zip(weights, path_2))\n",
        "    reward = score_2 - score_1 if action == 0 else score_1 - score_2\n",
        "    return max(-100, min(reward, 100))\n",
        "\n",
        "# Define a function to sample a random MDP\n",
        "def sample_mdp():\n",
        "    return NetworkEnv(dataset)\n",
        "\n",
        "# Define custom environment\n",
        "class NetworkEnv(gym.Env):\n",
        "    def __init__(self, data):\n",
        "        super(NetworkEnv, self).__init__()\n",
        "        self.data = data\n",
        "        self.current_step = 0\n",
        "        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(3,), dtype=np.float32)\n",
        "        self.action_space = spaces.Discrete(2)\n",
        "\n",
        "    def step(self, action):\n",
        "        current_data = self.data.iloc[self.current_step]\n",
        "        path_1 = [current_data['P1_RTT'], current_data['P1_CWND'], current_data['P1_inflight']]\n",
        "        path_2 = [current_data['P2_RTT'], current_data['P2_CWND'], current_data['P2_inflight']]\n",
        "        reward = calculate_reward(path_1, path_2, action)\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= len(self.data)\n",
        "        new_state = np.array(path_1 if action == 0 else path_2, dtype=np.float32)\n",
        "        return new_state, reward, done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "        first_row = self.data.iloc[self.current_step]\n",
        "        return np.array([first_row['P1_RTT'], first_row['P1_CWND'], first_row['P1_inflight']], dtype=np.float32)\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        pass\n",
        "\n",
        "# Define a simple policy network\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.softmax(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "# Define the MAML algorithm with allow_unused flag\n",
        "class MAML:\n",
        "    def __init__(self, policy, alpha, beta, num_inner_steps):\n",
        "        self.policy = policy\n",
        "        self.alpha = alpha  # Task-specific learning rate\n",
        "        self.beta = beta    # Meta-update learning rate\n",
        "        self.num_inner_steps = num_inner_steps\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=self.beta)\n",
        "\n",
        "    def adapt(self, task, initial_params):\n",
        "        adapted_params = initial_params\n",
        "\n",
        "        # Perform task-specific adaptation (inner loop)\n",
        "        for _ in range(self.num_inner_steps):\n",
        "            # Get a trajectory from the task using the standalone function\n",
        "            states, actions, rewards, _ = sample_trajectory(task, self.policy)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = self.compute_loss(states, actions, rewards, adapted_params)\n",
        "\n",
        "            # Compute gradients with allow_unused=True\n",
        "            grads = torch.autograd.grad(loss, adapted_params, create_graph=True, allow_unused=True)\n",
        "\n",
        "            # Update parameters using gradient descent\n",
        "            adapted_params = [param - self.alpha * grad if grad is not None else param\n",
        "                              for param, grad in zip(adapted_params, grads)]\n",
        "\n",
        "        return adapted_params\n",
        "\n",
        "    def compute_loss(self, states, actions, rewards, params):\n",
        "        log_probs = []\n",
        "        for state, action in zip(states, actions):\n",
        "            action_probs = self.policy(state)\n",
        "\n",
        "\n",
        "            action_probs = action_probs.squeeze()\n",
        "            if len(action_probs.shape) == 0:\n",
        "                action_probs = action_probs.unsqueeze(0)\n",
        "\n",
        "            log_prob = torch.log(action_probs[action])\n",
        "            log_probs.append(log_prob)\n",
        "\n",
        "        log_probs = torch.stack(log_probs)\n",
        "        rewards = torch.FloatTensor(rewards)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = -torch.sum(log_probs * rewards)\n",
        "        return loss\n",
        "\n",
        "    def meta_update(self, tasks):\n",
        "        meta_loss = 0\n",
        "        total_rewards = 0  # To accumulate rewards across tasks\n",
        "\n",
        "        for task in tasks:\n",
        "            initial_params = list(self.policy.parameters())\n",
        "\n",
        "            # Adapt the policy to the task\n",
        "            adapted_params = self.adapt(task, initial_params)\n",
        "\n",
        "            # Get a new trajectory with the adapted policy\n",
        "            states, actions, rewards, _ = sample_trajectory(task, self.policy)\n",
        "\n",
        "            # Compute the loss for the adapted policy\n",
        "            loss = self.compute_loss(states, actions, rewards, adapted_params)\n",
        "\n",
        "            # Accumulate the meta-loss\n",
        "            meta_loss += loss\n",
        "\n",
        "            # Calculate the total reward for the task\n",
        "            total_rewards += sum(rewards)  # Sum of rewards for this task\n",
        "\n",
        "        # Perform meta-update using the accumulated meta-loss\n",
        "        self.optimizer.zero_grad()\n",
        "        meta_loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Print the average reward for this meta-update\n",
        "        avg_reward = total_rewards / len(tasks)\n",
        "        print(f\"Average reward for this meta-update: {avg_reward:.2f}\")\n",
        "\n",
        "    def train(self, tasks, num_meta_iterations):\n",
        "        for iteration in range(num_meta_iterations):\n",
        "            print(f\"Meta iteration {iteration + 1}/{num_meta_iterations} starting...\")\n",
        "            self.meta_update(tasks)\n",
        "            print(f\"Meta iteration {iteration + 1}/{num_meta_iterations} complete\")\n",
        "\n",
        "# Define a function to sample a trajectory from the environment\n",
        "def sample_trajectory(env, policy):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    states, actions, rewards = [], [], []\n",
        "\n",
        "    while not done:\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "        action_probs = policy(state_tensor).detach().numpy().squeeze()\n",
        "        action = np.random.choice(len(action_probs), p=action_probs)\n",
        "\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        states.append(state_tensor)\n",
        "        actions.append(action)\n",
        "        rewards.append(reward)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "    return states, actions, rewards, done\n",
        "\n",
        "# Helper function to sample a batch of tasks\n",
        "def sample_tasks(num_tasks, dataset):\n",
        "    tasks = []\n",
        "    for _ in range(num_tasks):\n",
        "        tasks.append(NetworkEnv(dataset))\n",
        "    return tasks\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = 3  # State size (3: RTT, CWND, Inflight)\n",
        "hidden_size = 128\n",
        "output_size = 2  # Action size (2 paths)\n",
        "alpha = 0.01  # Inner loop learning rate\n",
        "beta = 0.001  # Outer loop (meta-update) learning rate\n",
        "num_inner_steps = 5\n",
        "num_meta_iterations = 50  # Number of meta-training iterations\n",
        "num_tasks = 10  # Number of tasks to sample for meta-training\n",
        "\n",
        "# Initialize environment, policy, and MAML\n",
        "policy = PolicyNetwork(input_size, hidden_size, output_size)\n",
        "maml = MAML(policy, alpha, beta, num_inner_steps)\n",
        "\n",
        "# Sample tasks and train MAML\n",
        "tasks = sample_tasks(num_tasks, dataset)\n",
        "maml.train(tasks, num_meta_iterations)"
      ]
    }
  ]
}